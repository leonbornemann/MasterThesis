\chapter{Basic Terminology and Definitions}
\label{chapter_definitions}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter3/Figs/Raster/}{Chapter3/Figs/PDF/}{Chapter3/Figs/}}
\else
    \graphicspath{{Chapter3/Figs/Vector/}{Chapter3/Figs/}}
\fi

This chapter establishes the terminology and basic formal definitions for this thesis and defines the problem of mining predictive episodes in a semi formal way (TODO: do that!).

\section{Complex Pattern Mining: Basic Definitions}

The following definitions are not taken from the previously mentioned event processing glossary, but will still be central for the agenda of this thesis.

\begin{mydef}
\textbf{Low Level Event} A Low Level Event $l$ is defined as a tuple $l= (t,V)$, where $t \in \mathbb{N}^+$ is the timestamp of its occurance and $V=(v_1,...,v_k)$ with $v_i \in S_i$ is a vector of values reported with the event. $S_i$ is a set of all possible values for the position $i$ which we will not narrow down further here since this obviously depends on the application domain. Common data types would of course be real numbers or categorical values.
\end{mydef}

Low level events are essentially event objects (as defined above), we just demand that their properties must be expressed as a tuple.

\begin{mydef}
\textbf{Annotated Event} An annotated event $e$ is defined as a tuple $e=(t,T)$, where $t \in \mathbb{N}^+$ is the timestamp and $T \in \Sigma$ is its derived event type. $\Sigma$ is the set of all possible event types, which will sometimes also be referred to as the event type alphabet.
\end{mydef}
There are a few things to note here. Annotated events are still basic event objects, however we are not interested in their specific properties except for their type label, which is what makes them annotated.
Furthermore this definition as well as the definition of low level events assumes that events are instantaneous. This is not always the case, but we limit the focus of this thesis to the mining of instantaneous events. There are other variants of event definitions that also allow for events to have a duration. %(TODO: cite sequences of temporal intervals?).
Given these defintions we can start defining what streams of events look like:
\begin{mydef}
\textbf{Low Level Event Stream} A Low Level Event Stream $S_L$ of length $n$ is defined as a sequence $S_L=(l_1,...,l_n)$, where all $l_i$ are low level events and given two events $l_i=(t_i,V_i)$ and $e_j=(t_j,V_j)$ we know that $i \leq j \implies t_i \leq t_j$ (the stream is sorted according to the timestamps).
\end{mydef}
The definition of an Annotated Event Stream is very similar:
\begin{mydef}
\textbf{Annotated Event Stream} An Annotated Event Stream $S_A$ of length $n$ is defined as a sequence $S_A=(e_1,...,e_n)$, where all $e_i$ are annotated events and given two events $e_i=(t_i,A)$ and $e_j=(t_j,B)$, where $A,B \in \Sigma$ and we know that $i \leq j \implies t_i \leq t_j$.
\end{mydef}
%The alert reader will have noticed that we assume a total order in the annotated event stream, since we forbid that two consecutive timestamps may have the same value. This means that we do not allow two events to occur concurrently within the stream. For non-distributed streams this basically no restriction, since the events are ordered anyway. If we face a distributed stream (for example in a sensor network) however it might become tricky to fulfill this constraint, since clock synchronization might be an issue. This property is by no means central to the algorithms we will consider, but it does help when analyzing theoretical properties, which is why we assume it for now. \newline
These definition somewhat simplify reality, since they assume that the stream has a certain length, in other words the stream is finite. This is not problematic if we can assume the following:

\begin{itemize}
	\item $n$ always contains the number of all elements we have seen so far
	\item $n$ gets updated whenever we read a new event
\end{itemize}

However we need to keep this restriction in mind, since the size of the stream will become relevant when determining frequency of complex events.
Given these two definitions we can give a general definition of the first step of the basic problem setting: the transformation of a low level event stream to an annotated event stream as a function:

\begin{mydef}
\textbf{Transformation Function} A transformation function is a function that takes a low level event $S_L$ as an input and maps it to a corresponding annotated event stream $S_A$.
\end{mydef}
Obviously this is a very broad definition, since in most cases this function is completely dependent on the domain as well as target event type alphabet and thus by extension the complex events the user is interested in. The concrete transformation function though is very important for the whole mining process, since if the transformation is done incorrectly or not in an optimal way the results of the mining algorithms that look the annotated event stream are likely to be unsatisfying or misleading. When designing a transformation function one has to deal with a lot of issues, for example whether to use aggregation techniques and if yes how and at which granularity. These issues are discussed in detail in (TODO: discuss in a later chapter and refer to it). \newline
%There are however general methods to automatically transform tuples of real values to annotated events by using bagging (TODO: include a source for that). These methods create the target event type alphabet on the fly (depending on the input types). They can be useful if we know nothing about the semantics of the low level data or if there is no clear event type alphabet as the target.
There is one more general term that is relevant for the context of this thesis. Most of the time we are not interested in the whole stream, but instead only look at a small window:

\begin{mydef}
\textbf{Time Window} Given an annotated event stream $S$ we define the Time Window $W(S_A,q,r)$ with $q,r \in \mathbb{N}^+$ and $q < r$ as the subsequence of $S_A$ that includes all events of the annotated event stream $S_A$ that have a timestamp $t$ where $q \leq t\leq r$. We call $w = r-q+1$ the size of Window $W$.
\end{mydef}

In the following, whenever we speak of a stream without any further context we refer to an annotated event stream (likewise we will often only use $S$ instead of $S_A$ to denote an annotated event stream formally).
%TODO: where to put this:

%The form of the ontology is not further specified in this definition since it is very user specific. Common knowledge representations in the area of semantic web technologies are of course RDF-Graphs. The precice process of enriching the data will be discussed later in section TODO.
%Fortunately the definition is still sufficient for a general purpose episode mining algorithm, since that algorithm does not require domain knowledge, it just requires the high level event stream. The following definitions are mostly inspiered by \cite{generatingDiverse}.

\section{Event Episodes}

Given an annotated stream there are multiple ways to define episodes of events. We will give two different definitions, first a very compact and formal definition and second a longer definition that is a bit more graphical and less formal.

\begin{mydef}
\textbf{Episode} An event episode (also sometimes called episode pattern) $E$ of length $m$ (also called m-Episode) is defined as a triple: $E = (N_E,{\leq}_{E},g_E)$ where $N_E = \{n_1,...,n_m\}$ is a set of nodes, ${\leq}_{E}$ is a partial order over $N_E$ and $g_E : V_E \rightarrow \Sigma$ is a mapping that maps each node of $N_E$ to an event type. If ${\leq}_{E}$ is a total order we call $E$ a serial episode, if there is no ordering at all we call $E$ a parallel episode. Otherwise we speak of composite episodes.
\end{mydef}

If we want to denote quick, simple episodes formally, we will use $\rightarrow$ as the sequence (ordering) operator. To show that there is no order specified between two episodes we use $\|$ as the parallel operator. For example $(A \, \| \, B ) \rightarrow C$ denotes a composite episode of length 3, which specifies that it does not matter in which order $A$ or $B$ occur, but $C$ must occur after both $A$ and $B$. If we want to discuss more complex episodes we will visualize them graphically like in figure TODO \newline  
It is helpful to think of episodes as a template or pattern for concrete occurrences, which we define next:

\begin{mydef}
\textbf{Episode Occurrence} An event episode $E = (N_E,{\leq}_{E},g_E)$ is said to occur in a window $W$ if events of the types that the nodes in $N_E$ are mapped to occur in $W$ in the same order that they occur in the episode. More formally if we are given a sequence of events $S=((t_1,T_1),...,(t_n,T_n))$ (which may be a Time Window, aka. a subsequence of the original stream) we can define an occurrence of $E$ as an injective Map $h:N_E \rightarrow \{1,...,n\}$, where $g_E(n_i) = T_{h(n_i)}$ and $\forall \, v,w \in V_E : v \;{\leq}_{E}\; w \implies t_{h(v)} \leq t_{h(w)}$ holds.
\end{mydef}

For example given the high level event stream $S = [ (12,A) , (14,B) , (19,C) , (22,A), (34,D) ]$ the Episode $E = B \rightarrow A$ occurs in window $W(S,14,22)$. \newline \newline 

TODO: include the more simple Definition. \newline \newline
%For now I provide the Link to the paper where the definition is easier to understand (but of course equivalent): http://infolab.stanford.edu/~ullman/mining/episodes.pdf \newline \newline


\section{Frequency of Episodes}
Defining frequency of episodes is surprisingly complex, since different definitions have a considerable impact on potential algorithms. Thus many different definitions exist, which will be briefly presented in this section. The first two definitions, which we will refer to as window based frequency and minimal occurance based frequency, are mentioned by Zimmermann, when he presents his method for synthetic episode generation \cite{zimmermann2012generating}, but were originally conceived in (TODO: include original sources). We refer to the third definition as the non-overlapping occurrence based Frequency which was suggested by Laxman et al. \cite{laxman2007fast}.
\begin{mydef}
\textbf{Episode Frequency - Window based Definition} Given a high level event stream $S$, a fixed window size of $w$ and an Episode $E$, we define the window based frequency $w\_freq(E)$ as the number of windows $W$ with size $w$ of $S$ in which $E$ occurs: $w\_freq(E) = |\,\{W(S,q,r) \mid r-q+1 = w \land E \;occurs\; in\; W \}\,|$.
\end{mydef}

This definition can be confusing at first since it is intended that episode occurrences that are comprised of the exact same events count just as many times as there are windows in which the events appear. If we have a window size of $w=11$ for the previously mentioned example, we can find the Episode \textit{B after A} in the consecutive windows $W(S,12,22)$, $W(S,13,23)$ and $W(S,14,24)$, which means we will get a frequency of $3$ just for the two events $(14,B)$ and $(22,A)$. This effect obviously increases with the window size. \newline
The second definition does not require a fixed window size to be specified but instead uses the concept of minimal occurrences:

\begin{mydef}
\textbf{Minimal Occurrence} An event episode $E$ is said to occur minimally in a window $W(S,q,r)$ if $E$ occurs in $W$ and there is no subwindow of $W$ in which $E$ also occurs. In this context we also refer to the window $W$ itself as a minimal occurrence of $E$.
\end{mydef}

\begin{mydef}
\textbf{Episode Frequency - Minimal Occurrence based Definition} Given a high level event stream $S$ and an Episode $E$, we define the minimal occurrence based frequency $mo\_freq(E)$ as the number of minimal occurrences of $E$ in $S$.
\end{mydef}

The third definition introduces the concept of non-overlapping occurrences:

\begin{mydef}
\textbf{Non-Overlapping Occurrences} Given a m-Episode $E = (N_E,{\leq}_{E},g_E)$ where $N_E = \{n_1,...,n_m\}$, two occurrences $h_1$ and $h_2$ of $E$ are non-overlapped if either 
\begin{itemize}
	\item $\forall \, n_j \in N_E : h_2(n_1)>h_1(n_j)$ or 
	\item $\forall \, n_j \in N_E : h_1(n_1)>h_2(n_j)$
\end{itemize}
A set of occurrences is non-overlapping if every pair of occurrences in it is non-overlapped.
\end{mydef}

This leads to the Definition:

\begin{mydef}
\textbf{Episode Frequency - Non-Overlapping Occurrences based Definition} Given a high level event stream $S$ and an Episode $E$, we define the non-overlapping occurrence based frequency $noo\_freq(E)$ as cardinality of the largest set of non-overlapped occurrences of $E$ in $S$ \cite{laxman2007fast}.
\end{mydef}



When looking at these definitions it is not clear whether any of these is always superior to or more useful than the other since they have different properties. We mention them briefly:

\begin{itemize}
	\item As already mentioned in the above example. The window based frequency counts an episode occurrence that is comprised of the same events in multiple windows. This might especially distort the count if the window size is high and the events in the episode happen with minimal delay between them.
	\item The minimal occurrence based definition of frequency does not suffer from the problem of the previous point
	\item The window based definition has the advantage that it already incorporates a fixed size during which episodes may occur, meaning there can not be episodes that stretch over a time period larger than the fixed window size $w$. This might be beneficial for potential algorithms, since it reduces the search space for episodes. On top of that it is also closer to reality, since episodes normally happen within a small time window \cite{generatingEpisodeDatasets}. Of course also the minimal occurrence based definition can be extended to incorporate a maximal time span.
	\item Both definitions do not consider multiple occurrences of an episode in the same time window.
\end{itemize}

TODO: include more definitions, possibly my own. \newline \newline
%TODO: move the above to a separate chapter. Due to the central importance of these definitions we will discuss and compare them in detail in section TODO. \newline \newline
These definitions are relevant when mining frequent episodes from databases or streams. We are however not specifically interested in all frequent episodes in thesis but instead focus on predictive episodes: TODO: define predictive Episodes.

