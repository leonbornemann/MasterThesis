\chapter{Suggested Algorithms}
\label{chapter_solutions}

% **************************** Define Graphics Path **************************
\ifpdf
    \graphicspath{{Chapter4/Figs/Raster/}{Chapter4/Figs/PDF/}{Chapter4/Figs/}}
\else
    \graphicspath{{Chapter4/Figs/Vector/}{Chapter4/Figs/}}
\fi

This chapter presents the suggested learning algorithms for predictive models in detail. Section \ref{sec_predictiveEpisodeMining} explains predictive episode mining, an approach that aims to discover predictive episodes in the stream. Section \ref{sec_FeatureBasedStreamWindowClassification} explains the idea of feature based stream window classification in order to predict event occurrences. 

\section{PERMS - Predictive Episode Rule Mining in Streams}
\label{sec_predictiveEpisodeMining}
The first suggested algorithm is called the PERMS algorithm, short for \textbf{P}redictive \textbf{E}pisode \textbf{R}ule \textbf{M}ining in \textbf{S}treams.

\subsection{Basic Ideas and Definitions}
In order to fully explain the algorithm we first need to define what we mean by predictive episode rules.

\begin{mydef}
\label{def_predictiveEpisode}
\textbf{Predictive Episode Rule} An episode $\alpha$ is called a predictive episode rule for event $A \in \Sigma$ if $\alpha$ has the form $\beta \rightarrow A$ where $\beta$ is an episode. 
\end{mydef}

In the context of a predictive episode rule $\alpha = \beta \rightarrow A$ we also refer to $\beta$ as the prefix and to $A$ as the suffix or target event of $\alpha$. 
The basic idea of the prediction algorithm using predictive episode rules is rather simple. If given a set $P$ of predictive episode rules, we monitor the stream and whenever we detect the prefix of an episode in $P$ we predict an occurrence of $A$. The idea is similar to the mining of sequential rules or rule-based classification (TODO: cite).
Of course there is an infinite amount of predictive episode rules for an event type, finding those predictive episode rules that are actually useful for predicting the occurrence of an event is the main task when building (training) the model. The use case here is very similar to the related work that deals with the constraint based mining of episode rules (TODO: cite), where episode rules are mined in order to discover dependencies between earthquakes. There are two main important differences that make the approach used by the authors (TODO: name them) impractical.

\begin{itemize}
 \item We are interested in episode rules of one specific type (those predicting the desired event $A$), thus it is not necessary to mine all frequent episode rules in the stream.
 \item We are dealing with data in a streaming environment, meaning we can neither analyze the whole data, nor make multiple passes over the entire stream.
\end{itemize}

In order to describe how the set $P$ is built by the suggested learning algorithm,we first need to define frequency, support and confidence for predictive episode rules, which are similar to classic association rules(TODO: cite). Before this can be done however we need to determine which frequency definition we will use in this algorithm. 

\subsection{Choice of Frequency Measure}

Recall that there are three main frequency measures that were proposed in the literature:

\begin{itemize}
	\item The Window-based Frequency (see defintion \ref{def_windowBasedFrequency})
	\item The frequency based on minimal occurrences (see definition \ref{def_minimalOccuranceFrequency})
	\item The frequency based on non-overlapping occurrences (see definition \ref{def_nonOverlappingFrequency})
\end{itemize}

Their properties were already discussed in subsection \ref{subsec_otherFrequency}. While the frequency measure using the non-overlapping occurrences is the latest measure and offers the best theoretical runtime, when recognizing episodes in a sequence it has a few drawbacks that are detrimental to its use in the given scenario. The first one is that it does not limit the duration of the episode occurrences. However limiting the duration of episode rules, or in other words giving episode occurrences expiry times (TODO: cite) is necessary when predicting event occurrences in streams. Consider for example the simple predictive episode rule $\alpha = B \rightarrow A$ and the following example sequence: 

\begin{equation}
S = [ (B,1),(C,2),(A,3),(B,4),...,(A,2000) ] 
\end{equation}

The non-overlapping frequency definition recognizes both $(B,1) \rightarrow (A,3)$ and $(B,4) \rightarrow (A,2000)$ as equally valid occurrences of $\alpha$ in $S$. However it is to be expected that when looking for episode rules for predictive purposes in a stream the events should happen close to each other (time-wise). This means that in this case $(B,1) \rightarrow (A,3)$ is likely a correct occurrence and prediction, whereas the occurrence $(B,4) \rightarrow (A,2000)$ is simply owed to the fact that at some point of time event $A$ will occur again in the stream and $(B,4)$ happened to occur a long time before that, without there necessarily being a causality.
Additionally the non-overlapping frequency assumes that there is one long sequence, from which the episodes are to be mined. The window-based frequency however works on a set of individual windows of a sequence or stream. In the previous work this had not been an advantage or disadvantage, since it did not focus on the streaming scenario, which meant that it was possible to analyze the entire sequence. This lead to the use of sliding windows over the sequence when using the window-based frequency. In the streaming scenario it is impossible to use all of the data, instead a certain selection is necessary. This can be done easily when using the window-based frequency by simply storing the windows that are of interest. Selecting data for the non-overlapping frequency definition is more difficult. Thus we will use the window-based frequency in this approach

\subsection{Basic Definitions}
Since we decided on a frequency measure it is now possible to define frequency, support and confidence of predictive episode rules.

\begin{mydef}
\label{def_frequency}
\textbf{Frequency} If given a set of time windows $WIN$ (see definition \ref{def_timeWindow}) of a sequence the frequency of a predictive episode $\alpha$ is defined as the number of windows in which $\alpha$ occurs: $freq(\alpha) = |\{W\,|W \in WIN\; \land \alpha\; occurs \; in \; W\}|$
\end{mydef}

\begin{mydef}
\label{def_support}
\textbf{Support} If given a set of time windows $WIN$ of a sequence the support of a predictive episode $\alpha$ is defined as $s(\alpha) = \frac{freq(\alpha)}{|WIN|}$ TODO: source
\end{mydef}

\begin{mydef}
\label{def_confidence}
\textbf{Confidence} The confidence of a predictive episode $\alpha = \beta \rightarrow A$ is defined as $c(\alpha) = \frac{freq(\alpha)}{freq(\beta)}$ TODO: source
\end{mydef}

The intention is clear and similar to the mining of classic association rules (TODO: cite): If a rule has a high confidence, it means that the prefix of the rule rarely occurs without its suffix $A$, meaning there is a high chance that this is a true predictor for the event $A$. Thus the goal of PERMS is to find a set of predictive episode rules $P$ that have a very high confidence and are above a certain support limit.

\subsection{Choice of training data}
Recall that in streaming applications it is impossible for to analyze the whole stream as one sequence using multiple passes. Thus, when presented with a stream any prediction or forecasting algorithm first needs to take some time to study the stream and extract training data to build the model. So if given an annotated event stream, how do we determine the training data? As it was described in section \ref{sec_episodeMiningBackground} the data basis for episode mining is one very long sequence. Thus the first simple approach would be to simply record the stream as a sequence until we have reached a number of desired elements or run out of memory. The recorded sequence would then be the training sequence from which the predictive episode rules can be mined. This approach is visualized in figure \ref{fig_trainingDataNaive}.

\begin{figure}[h]
	\centering
  	\includegraphics[width=\textwidth]{trainingDataNaive}
	\caption{Visualization of a simple split of the stream into a training segment at the beginning, followed by a (potentially endless) test phase}
	\label{fig_trainingDataNaive}
\end{figure}

The disadvantage of this naive approach is that there is no guarantee about the number of occurrences of the event we aim to predict. Say we want to predict $A$ and $A$ is rather sparse in the beginning of the stream, then we will have a very small data basis to extract predictive episode rules for $A$ and thus will likely not succeed. 
A better approach is to scan the stream for occurrences of the event $A$ and whenever an event of type $A$ enters the window we store the current window in a list until we have a sufficient number of windows. This approach is visualized in figure \ref{fig_trainingDataWindowsOfA}. This approach guarantees that we have a sufficient number of windows that are immediately followed by the target event. The mining process then is to simply find frequent episodes in the mined windows. Each of these then automatically corresponds to the prefix of a potential predictive episode rule with the suffix being the target event ($A$ in the figures).

\begin{figure}[h]
	\centering
  	\includegraphics[width=\textwidth]{trainingDataWindowsOfA}
	\caption{Visualization of using fixed windows that precede the target event as training examples. Predictive episodes can be mined from the windows that are extracted from the stream as shown above.}
	\label{fig_trainingDataWindowsOfA}
\end{figure}

The obvious and very big disadvantage is that there are no negative examples in the training sample taken from the stream. Each window is immediately followed by the target event $A$, thus every episode mined from the windows can have $A$ appended as a suffix and thus every predictive episode rule will have a confidence of $1.0$. This means that selection via confidence is meaningless, since we have seen no negative examples, meaning windows that are not followed by $A$. However negative examples can be extracted from the stream in a similar manner. This is the basic idea behind the training data selection in PERMS. It is visualized in figure \ref{fig_trainingDataPositiveAndNegativeWindows}. 

\begin{figure}[h]
	\centering
  	\includegraphics[width=\textwidth]{trainingDataPositiveAndNegativeWindows}
	\caption{Extracting positive and negative example windows from the stream.}
	\label{fig_trainingDataNaive}
\end{figure}

There are still some issues with this approach that can be detrimental to the predictive performance of the model. One issue is that, in the simple standard scenario extract an equal amount of positive and negative windows from the stream with no respect to the original distribution. If $A$ occurs very rarely, having an equal amount of positive and negative examples in the training data does not reflect the original event distribution in the stream. If that is the case, this can be fixed by including a number of negative examples that is proportionate to the original distribution (which is either known or learned while extracting the training data). However it is unclear if that would actually have a significantly positive effect on the performance of the resulting model.



TODO: define negative and positive windows!




\subsection{PERMS Parameters and Pseudocode}
\label{subsec_perms}

The PERMS algorithm uses the following user-defined parameters:

\begin{itemize}
	\item \textbf{$d$} - the (temporal) size of the sliding window. This also implies that all windows which the predictive episode rules will be mined from will exactly have duration $d$. TODO: talk about episode duration?	
	\item \textbf{$m$} - the number of windows to mine the predictive episode rules from. Basically the sample size we take from the stream.
	\item \textbf{$s$} - the minimum support that predictive episode rules must have to be considered for the model (rules with support $s$ or higher are frequent).
	\item \textbf{$n$} - the desired size of the final set of predictive episodes.
\end{itemize}

The pseudocode for PERMS is given in algorithm \ref{alg_PERMS}. 


\begin{algorithm}[H]
  \caption{PERMS
    \label{alg_PERMS}}
  \begin{algorithmic}[1]
    \Statex
    \Require Let $S=[(T_s,t_1),...]$ be a stream of events and let $d$,$m$,$s$,$n$ be the parameters as defined in the beginning of subsection \ref{subsec_perms}.
    \Function{PERMS}{}
      \Let{$(PE,NE)$}{$WindowMining(S,d,m,A)$} 
      \Let{$P$}{$BuildPredictiveEpisodeModel(PE,NE,s,n)$}
      \State apply Set of predictive Episodes $P$ to the rest of the Stream $S$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

As seen in the algorithm above the basic idea of the PERMS algorithm can be divided into 2 parts:

\begin{itemize}
	\item Training Example Mining (Window Mining)
	\item Predictive Episode Model Building
\end{itemize}

\textbf{Training Example Mining}\\
We start at the beginning of the stream and keep a sliding window of a user defined size $d$ in memory (see definition \ref{def_timeWindow}. Whenever an event of type $A$ enters the window we store the current window in a list until we have a sufficient number of windows ($m$). Additionally we also store $m$ windows that do not contain $A$ and were also not followed by $A$ in the near future. The pseudocode for this is given in algorithm \ref{alg_traningExampleMining}. TODO: negative examples genauer erklaeren.

\begin{algorithm}[H]
  \caption{Training Example Mining
    \label{alg_traningExampleMining}}
  \begin{algorithmic}[1]
    \Statex
    \Require Let $S=[(T_s,t_1),...]$ be a stream of events, $d$ be the window size, $m$ the required number of windows and $A$ the event type to predict.
    \Function{WindowMining}{}
      \Let{$PE$}{$\emptyset$} 
      \Let{$NE$}{$\emptyset$}
      \Let{$i$}{$1$}
      \Let{$t_{last}$}{$t_1$}
      \While{$|PE| \neq m \; \lor |NE| \neq m $}
      	\Let{$(T_i,t_i)$}{$S[i]$}
      	\If{$T_i = A$}
      		\Let{$t_{last}$}{$t_i$}
      		\If{$|PE| \neq m$}
          		\Let{$PE$}{$PE \cup W(S,i-d-1,i-1)$} \Comment{Add backwards window of size $d$ to training examples, including every event before timestamp $i$}
          	\EndIf
		\ElsIf{$|NE| \neq m \; \land t_i - t_{last} \geq d\cdot 2$}
			\Let{$NE$}{$NE \cup W(S,i-d \cdot 2,i-d)$}
			\Let{$t_last$}{$t_i$}
       	\EndIf
       	\Let{$i$}{$i+1$}
      \EndWhile
      \State \Return{$(PE,NE)$}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\textbf{Predictive Episode Model Building}\\

Once we have enough training examples, we can start to mine serial and parallel episodes from the windows preceding $A$ that have support $s$ or higher. For that purpose we employ the general mining algorithm for frequent episodes, (see algorithm \ref{alg_generalEpisodeMining} in subsection \ref{subsec_episodeDiscovery} ). The existing algorithms for counting episode frequency are made for the use case of one long sequence being mined for frequent patterns instead of single windows. Thus they are rather complex, since they avoid recounting the frequency of a candidate episode for each window and instead update counts when sliding the window forward (TODO: cite). Since the windows in our use case are not necessarily back to back, we can simplify the episode detection algorithms. The simplified detection algorithms for serial and parallel episodes are shown in algorithms TODO and TODO. Afterwards we rank the discovered rules by confidence, keep the $|P|$ episodes with the highest confidence and return them as the set of predictive episodes $P$. The pseudocode for this is given in algorithm \ref{alg_BuildPredictiveEpisodeModel}.

\begin{algorithm}[H]
  \caption{Predictive Episode Model Building
    \label{alg_BuildPredictiveEpisodeModel}}
  \begin{algorithmic}[1]
    \Statex
    \Require Let $PE$ be the positive examples (Windows preceding $A$), let $NE$ be the negative examples (Windows not followed by $A$. Furthermore let $s$ be the minimum support and $n$ the number of predictive episodes to keep.
    \Function{BuildPredictiveEpisodeModel}{}
      \Let{$P$}{$MineFrequentParallelEpisodes(PE,s)$} 
      \Let{$S$}{$MineFrequentSerialEpisodes(NE,S$}
      \Let{$E$}{$P \cup S$}
      \State Sort the $\alpha \in E$ by  $\alpha .c$
      \State \Return{the top $n$ episodes $\alpha \in E$}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

TODO: is the following necessary? I am not sure - review related work algorithms again!

The detection of serial episodes in a single window can be implemented in the following:

\begin{algorithm}[H]
  \caption{Serial Episode Detection
    \label{alg_SerialEpisodeDetection}}
  \begin{algorithmic}[1]
    \Statex
    \Require Let $W=[(T_s,t_1),...,(T_e,t_n)]$ be a Window of the Stream and let $\alpha$ be a serial episode.
    \Function{SerialEpisodeDetection}{}
      \Let{$i$}{$1$}
      \Let{$pos$}{$1$}
      \Let{$t_{prev}$}{$-1$}
      \While{$i \leq |W| \land pos \neq |\alpha |$}
      	\Let{$(T_i,t_i)$}{$W[i]$}
      	\If{$T_i = \alpha [pos] \; \land t_i \neq t_{prev}$}
      		\Let{$t_{prev}$}{$t_i$}
      		\Let{$pos$}{$pos+1$}
        \EndIf
       	\Let{$i$}{$i+1$}
      \EndWhile
      \State \Return{$true$ if $pos =|\alpha |$, $false$ otherwise}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

The detection of parallel episodes in a single window can be implemented in the following:

\begin{algorithm}[H]
  \caption{Parallel Episode Detection
    \label{alg_ParallelEpisodeDetection}}
  \begin{algorithmic}[1]
    \Statex
    \Require Let $W=[(T_s,t_1),...,(T_e,t_n)]$ be a Window of the Stream and let $\alpha$ be a parallel episode.
    \Function{ParallelEpisodeDetection}{}
      	\Let{$i$}{$1$}
      	\Let{$removed$}{$0$}
      	\Let{$Remaining$}{new empty Map}
		\For{each $A \in \alpha$}
      		\Let{$Remaining[A]$}{$count(\alpha,A)$}
      	\EndFor      
      \While{$i \leq |W| \; \land removed \neq |\alpha |$}
      	\Let{$(T_i,t_i)$}{$W[i]$}
      	\If{$T_i \in \alpha \;\land Remaining[T_i] \neq 0$}
      		\Let{$Remaining[T_i]$}{$Remaining[T_i] - 1$}
      		\Let{$removed$}{$removed + 1$}
        \EndIf
       	\Let{$i$}{$i+1$}
      \EndWhile
      \State \Return{$true$ if $removed =|\alpha |$, $false$ otherwise}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

TODO: detecting all episodes in one iteration over the window


After selecting the best $n$ predictive episodes the model can be applied to the rest of the stream.
The application of the predictive model to the stream is very simple: If an episode $\alpha \in P$, with $P$ being the set of best predictive episodes, occurs in the current sliding window output $1$ and otherwise output $0$ for the current sliding window.

%\begin{algorithm}[H]
%  \caption{Calculate Window based Frequency for parallel Episodes
%    \label{alg_windowBasedParallel}}
%  \begin{algorithmic}[1]
%    \Statex
%    \Require Let $C$ be the set of candidate parallel episodes, and let $S=[(T_1,t_s),...,(T_n,t_e)]$ be a sequence of events, let $win$ be the window size and finally let $minS$ be the minimum support.
%      \State \textit{// Initialization}
%      \For{each $\alpha \in C$}
%      	\For{each $A \in \alpha$}
%      		\Let{$A.count$}{$0$}
%      		\For{$i \in \{1,...,|\alpha |\}$}
%      			\Let{$contains(A,i)$}{$\emptyset$}
%      		\EndFor
%      	\EndFor
%      \EndFor
%      \For{each $\alpha \in C$}
%      	\For{each $A \in \alpha$}
%      		\Let{$a$}{number of events of type $A$ in $\alpha$}
%      		\Let{$contains(A,a)$}{$contains(A,a) \cup \{\alpha \}$}
%      	\EndFor
%      	\Let{$\alpha .eventCount$}{$0$}
%      	\Let{$\alpha .freq$}{$0$}
%      \EndFor
%      \State \textit{// Recognition}
%      \For{$start \gets t_s - win +1 \textrm{ to } t_e$}
%      	\State \textit{//Bring new events to the window}
%      	\For{each $(t,A) \in S $ where $t = start+win-1$}
%      		\Let{$A.count$}{$A.count +1$}
%      		\For{each $\alpha \in contains(A,A.count)$}
%      			\Let{$\alpha .eventCount$}{$\alpha .eventCount + A.count$}
%      			\If{$\alpha .eventCount = |\alpha | $}
%          			\Let{$\alpha .inwindow$}{$start$}
%       			\EndIf
%      		\EndFor
% 		\EndFor
% 		\State \textit{// Drop old events out of the window}
% 		\For{each $(t,A) \in S $ where $t = start-1$}
% 			\For{each $\alpha \in contains(A,A.count)$}
% 				\If{$\alpha .eventCount = |\alpha | $}
%          			\Let{$\alpha .freq$}{$\alpha .freq + \alpha .inwindow -start $}
%       			\EndIf
%       			\Let{$\alpha .eventCount$}{$\alpha .eventCount - A.count$}
% 			\EndFor
% 			\Let{$A.count$}{$A.count -1$}
% 		\EndFor
%      \EndFor
%      %\Let{$freq$}{$\emptyset$}
%      %\Let{$i$}{$1$}
%      %\While{$C_i \neq \emptyset$}
%      	%\State Count frequencies of each Episode $E \in C_i$
%       % \Let{$L_i$}{ $\{ E \mid E \in C_i \land C_i \; is\; frequent\}$}
%       % \Let{$freq$}{$freq \cup L_i$}
%       % \Let{$C_{i+1}$}{Generate Episode Candidates of length $i+1$ from $L_i$}
%        %\Let{$i$}{$i+1$}
%      %\EndWhile
%      \State \Return{$\{\alpha \mid \alpha \in C \land \frac{\alpha.freq}{t_e - t_s + win -1} \geq minS\}$}
%  \end{algorithmic}
%\end{algorithm}

\section{Feature Based Stream Window Classification}
\label{sec_FeatureBasedStreamWindowClassification}

The basic idea of the second approach called feature based stream window classification is similar to the one of perms. In fact the process of gathering training data as visualized in figure \ref{trainingDataPositiveAndNegativeWindows} is exactly the same. 


\section{Evolving the models with the stream}